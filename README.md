# achyuraacode

Data Preprocessing

The genres/keys were transformed into numeric values then the mode (which was categorical–”major” or “minor”) was one hot encoded. There were 5 rows in the original dataset that were completely empty and so they were dropped. Additionally, columns/features like artist name, track name, unique Spotify ID of each song, and the date the information was obtained from Spotify doesn’t necessarily impact the song’s genre and so they were dropped. As per the directions, the predictor data was split into training and test sets so there were 4500 songs from each genre in the training set and 500 in the test. Since the tempo column had missing values, a correlation matrix (made of r’s–pearson correlation coefficients that describes covariance, ranging from -1 to 1–negative to strong positive correlation) was used to find the features that are the most correlated to tempo for imputation. This turned out to be acousticness (which has an inverse correlation), and energy/loudness (all of these had a correlation coefficient > 0.2.) A random forest (of 100 full-grown trees) that used mean squared error as the loss function and was bootstrapped was fitted on the training set rows that had a value for tempo in the training set. This approach builds 100 trees that are trained on a random subset of the data–with some rows being repeated so each tree makes splits that reduce impurity the most earlier on by ordering feature splits by their impact on the mean squared error. All of the predictions (for each tree) are then averaged to find the output (tempo, in this case.) This reduces chances of overfitting and works efficiently with large datasets. In fact, since the splits would already be naturally sorted in a way to reduce deviation from the actual tempos found in the training set, it isn’t particularly necessary to use a correlation matrix (the model would determine this itself.) This trained model was then used to impute missing tempo information into both the training and test sets.

Dimensionality Reduction

Prior to dimensionality reduction, a linear support vector classifier was fitted on the training data to see if it was linearly separable in this higher dimensional space or had nonlinear relationships amongst its features. The accuracy of this model was 28.6% indicating that there was indeed nonlinearity present so a nonlinear dimensionality reduction was utilized (specifically, autoencoders.) MDS and t-SNE were avoided because of time complexity (since the dataset size is 50k.) Additionally, there was no measure of how important the data’s global structure is to genre-categorization so the model needed to preserve both global and local structure. UMAP was not utilized because of its sensitivity to noise/outliers (along with not having metrics to assess quality of the low-dimensional embedding it generated which was important since this low dimensional data was going to be used for classification afterwards.) To ensure none of the features had a greater influence on the predictions made by the autoencoder simply because of their scale, MinMaxScaler was utilized (which makes all of the data range from 0 to 1 afterwards and is also the reason why the output layer’s activation was sigmoid in all of the models tested.) 6 models with a differing bottleneck size (3 or 6) and number of hidden layers (with ReLU activation for efficiency and avoiding the vanishing gradient problem) were built to find which one would best reconstruct the data. Each autoencoder had a funnel shape (want to gradually merge features to learn more complex ones so the information loss is more minimal and want to gradually build up from the bottleneck so enough of the features are captured.) Both the training and test sets were divided into smaller batches so the parameters were updated frequently enough to learn meaningful relationships for each genre. After fitting each model on the training set (via mean squared error and adam optimizer), the test loss was additionally collected to see how well it was generalizing–especially because autoencoders are prone to overfitting. Each model’s performance was tracked across 200 epochs (to ensure it was converging and not underfitting.) As the number of layers grew, the performance deprecated indicating the data in higher dimensional space did not necessarily contain complex patterns/relationships. Although the best model was 6 dimensional, the autoencoder with the lowest training/test loss, the one with the lowest loss with a 3 dimensional bottleneck was utilized to have more interpretable clustering. However, this difference may have been great enough to deprecate overall model performance (6 having 0.00148 on training and 0.0021 on test with 3 having 0.0061 on training and 0.00734 on test as the mean squared error), especially since the clustered information was used to boost the classification model performance at the end. The training/test predictor data was reduced using the encoder part of this model.

Clustering

After reduction, kMeans/GMM were settled on for clustering since the data already provides a number of clusters the data may naturally be split in (10 genres.) Additionally, other methods such as DBSCAN are computationally very expensive for a dataset of 45k points (O(45k^2) = 2 billion operations.) However, the limitations of this approach is sensitivity to outliers and the assumption that the dimensionally reduced data follows a gaussian distribution (which was not true for the higher dimensional data–even more specifically with kMeans which within cluster data comes from one-dimensional gaussian distribution.) To determine parameters (the initialization technique for kMeans and GMM) and the covariance type for GMM (there was no prior information on covariance/variance by dimension), inertia and BIC were collected respectively over 10 trials for each unique value of the parameter to avoid trial bias (since both models can get stuck at local minima if initialization is not done optimally.) Random initialization had the lowest inertia for kMeans (meaning lowest distance for all points from their respective centroids.) kMeans and full covariance had the lowest BIC (measures the tradeoff between model’s ability to fit to data with its complexity. A lower number means the model predicts the underlying data distribution better while preventing overfitting.) Then, to compare both models, silhouette scores were utilized which and kMeans had a higher score meaning it maximed the distance of each datapoint from the next closest cluster while minimizing the distance from its own center as much as possible. The 3-dimensional data was then graphed. The separation between different clusters seems to be “clean” although there it does seem like there is more distortion for some clusters than others present as can be seen in the figure below.

Classification

Initially, a neural network was used for a few reasons. After dimensionality reduction via autoencoders, the dimensions that were extracted aren't easily interpretable. With lower interpretability on the data's dimensions, the understandability of the splits in a decision tree (arguably its most important advantage) is also reduced (regardless of whether it's in a random forest or there are boosting/bagging methods being utilized.) Additionally, data is not a limitation here but SVMs in particular don't natively handle multiclass classification. Making it handle such classification would add more training complexity. However, as the number of hidden units per layer/layers, learning rate, and activation function were altered, 3 observations (related to performance on both training and test sets) emerged. The best model was simply linear (no activation), a higher learning rate led to higher accuracy (perhaps because it helped avoid getting stuck at local minimas), and model performance only improved with more layers when there was no activation. This indicated that the data was linearly separable but the hyperplanes may not be simple (equation wise)--and was further supported by the fact that a higher number of hidden units per layer led to better performance for the no activation models. These insights led to pivoting to a linear support vector classifier. Initially, the labels from kMeans were not added to the dataset due to it being an unsupervised method so the assigned clusters may not have coincided with the actual class labels. However, after experimentation, it happened to enhance the classifier’s performance. A reason for this could be that kMeans may have uncovered structural relationships (that may not be inherently visible through just the reduced data but may indicate that certain songs have more aligned properties.) These labels were one hot encoded (falling into one category shouldn’t be weighed differently than any other.) The regularization method/constant and number of iterations were all altered. L2 happened to perform better, 2000 maximum iterations were used since the model wasn’t converging with some of the smaller numbers, and 10 seemed to be the best regularization constant (meaning tighter decision boundaries/greater penalization for misclassification.) AUC was found for each model which corresponds to the area under the ROC curve (goes from 0.5 to 1 with 0.5 corresponding to the model’s predictive power being no better than random guessing and 1 being that it perfectly distinguishes them.) The AUC hovered around 0.725 meaning the model had moderate predictive power. The lowest AUC was for “Anime,” “Alternative,” “Blues,” and “Rock.” In fact for both “Anime” and “Rock,” precision, recall, f1-score were all zero. F1-score (a harmonic mean of precision (fractions of predictions that came true) and recall (ability to detect a signal that is really present)) computes how many times the model made an accurate prediction across the dataset (regardless of the output’s categorization.) This means the model never correctly predicted these two genres. Or actually, through the confusion matrix, it never predicted that a song belonged to these two genres at all. But AUC is non-zero because in these cases, these genres may have been given a higher probability of being the label when it actually was and lower probability when it actually wasn’t. The ROCs graphed:

The most important factor for classification success was probably realizing that the lower dimensional data is now linearly separable with a larger probability than it having complex relationships–possibly because of getting rid of noise. I believed the opposite would be true especially because dimensionality reduction causes feature extraction (combining several into one, for example) so one would assume the abstraction in this new dimension leads to greater complexity. Typically, as can be seen with the kernel trick in SVMs, to find linear separability data is projected onto higher dimensional space and the results found were the opposite. In terms of the data, it would need to be that all of the information is equally relevant to all of the genres. Otherwise, any sort of reduction/clustering/classification would be heavily biased to perform a certain way per class.
